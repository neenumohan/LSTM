{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neenumohan/LSTM/blob/main/LSTM_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "#Download the dataset\n",
        "path=tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "#Explore the data\n",
        "text = open(path, \"r\").read()\n",
        "print(text[:200])\n",
        "# one-hot encode\n",
        "maxlen = 60 #extract sequences of length 60\n",
        "step = 3\n",
        "sentences = []  #holds extracted sequences\n",
        "next_chars = [] #holds the targets\n",
        "for i in range(0, len(text)-maxlen, step):\n",
        " sentences.append(text[i:i+maxlen])\n",
        " next_chars.append(text[i+maxlen])\n",
        "#VECTORIZATION\n",
        "chars = sorted(set(text))\n",
        "char_indices = dict((char, chars.index(char)) for char in chars)\n",
        "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
        "for i, sentence in enumerate(sentences):\n",
        " for t, char in enumerate(sentence):\n",
        "   x[i, t, char_indices[char]] = 1\n",
        " y[i, char_indices[next_chars[i]]] = 1\n",
        "from tensorflow.keras import layers\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(layers.LSTM(128, input_shape=(maxlen, len(chars))))\n",
        "model.add(layers.Dense(len(chars), activation=\"softmax\"))\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")\n",
        "def sample(preds, temperature=1.0):\n",
        " preds = np.asarray(preds).astype('float64')\n",
        " preds = np.log(preds) / temperature\n",
        " exp_preds = np.exp(preds)\n",
        " preds = exp_preds / np.sum(exp_preds)\n",
        " probas = np.random.multinomial(1, preds, 1)\n",
        " return np.argmax(probas)\n"
      ],
      "metadata": {
        "id": "YIE-hqfCz4PN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17ee7db8-c60c-4b95-9ce6-0c7a1f1a3b97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-762a335cdadf>:19: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
            "<ipython-input-1-762a335cdadf>:20: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #The following loop repeatedly trains and generates the text.\n",
        "import random\n",
        "for epoch in range(1, 60):\n",
        "  model.fit(x, y, batch_size=128)\n",
        "  start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "  generated_text = x[start_index: start_index + maxlen]\n",
        "for i in range(400):\n",
        "   preds = model.predict(generated_text)[0]\n",
        "   next_index = sample(preds,temperature)\n",
        "   next_char = chars[next_index]\n",
        "   generated_text += next_char\n",
        "   generated_text = generated_text[1:]\n",
        "   print(next_char)\n",
        "   print(generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMi2DgKNc1XS",
        "outputId": "a5623d00-4a2e-4674-8711-382230820e70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2905/2905 [==============================] - 27s 6ms/step - loss: 2.4163\n",
            "2905/2905 [==============================] - 20s 7ms/step - loss: 2.0216\n",
            "2905/2905 [==============================] - 18s 6ms/step - loss: 1.8792\n",
            "2905/2905 [==============================] - 18s 6ms/step - loss: 1.7913\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense"
      ],
      "metadata": {
        "id": "sb-5ztL7BZIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 1 : Read file lstm.txt\n",
        "with open(\"lstm.txt\") as f:\n",
        "    corpus = f.read()\n",
        "    corpus.replace('\\n', '')\n",
        "chars = list(corpus);print(chars)\n",
        "charCode = {char: i for i, char in enumerate(chars)};print(charCode)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FeDB7l7LBcQ7",
        "outputId": "2bef4b10-726c-4196-a40b-6c4c57a2c1da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['j', 'a', 'n', 'a', ' ', 'g', 'a', 'n', 'a', ' ', 'm', 'a', 'n', 'a', ' ', 'a', 'd', 'h', 'i', ' ', 'n', 'a', 'y', 'a', 'k', 'a', ' ', 'j', 'a', 'y', 'a', 'h', 'e']\n",
            "{'j': 27, 'a': 30, 'n': 20, ' ': 26, 'g': 5, 'm': 10, 'd': 16, 'h': 31, 'i': 18, 'y': 29, 'k': 24, 'e': 32}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 2: Produce training data from corpus, 4 chars input, next one character is output\n",
        "x1 = []; y1 = []\n",
        "for i in range(0,len(corpus)-4):\n",
        "    a = corpus[i:i+4];#print(a)\n",
        "    b = corpus[i+4]\n",
        "    x1.append([charCode[i]for i in a]);\n",
        "    y1.append(charCode[b])\n",
        "    x = np.array(x1);y = np.array(y1);  #convert to np arrays\n",
        "print(x1);print(y1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YpMiLrNBev3",
        "outputId": "8ed734a5-dd0c-4c0d-d6fc-58a0648e3c31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[27, 30, 20, 30], [30, 20, 30, 26], [20, 30, 26, 5], [30, 26, 5, 30], [26, 5, 30, 20], [5, 30, 20, 30], [30, 20, 30, 26], [20, 30, 26, 10], [30, 26, 10, 30], [26, 10, 30, 20], [10, 30, 20, 30], [30, 20, 30, 26], [20, 30, 26, 30], [30, 26, 30, 16], [26, 30, 16, 31], [30, 16, 31, 18], [16, 31, 18, 26], [31, 18, 26, 20], [18, 26, 20, 30], [26, 20, 30, 29], [20, 30, 29, 30], [30, 29, 30, 24], [29, 30, 24, 30], [30, 24, 30, 26], [24, 30, 26, 27], [30, 26, 27, 30], [26, 27, 30, 29], [27, 30, 29, 30], [30, 29, 30, 31]]\n",
            "[26, 5, 30, 20, 30, 26, 10, 30, 20, 30, 26, 30, 16, 31, 18, 26, 20, 30, 29, 30, 24, 30, 26, 27, 30, 29, 30, 31, 32]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 3: Determine the Training Model\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(chars),256,input_length=4))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dense(len(chars), activation = 'softmax'))\n",
        "model.compile(loss ='sparse_categorical_crossentropy',optimizer = 'adam')\n",
        "model.fit(x,y,batch_size = 128, epochs = 500)"
      ],
      "metadata": {
        "id": "MYP8kRiLBhJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 4: Predict\n",
        "\n",
        "seed = 'dhi '\n",
        "x = np.array([charCode[char] for char in seed[-4:]])\n",
        "x = np.reshape(x,(1,4))\n",
        "t = model.predict(x)\n",
        "out = np.argmax([i for i in t])\n",
        "print(\"output is : \",out)\n",
        "print(\"character:\", list(charCode.keys())[list(charCode.values()).index(out)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlYTzeOxBj7b",
        "outputId": "14a7a473-ddef-45c7-ea9d-4cc8ed53d82d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 17ms/step\n",
            "output is :  20\n",
            "character: n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import regex as re"
      ],
      "metadata": {
        "id": "e_IeVnEy_zmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = 'pizza.txt'\n",
        "text = open(path).read().lower()         # convert to lowercase and assign to a variable\n",
        "sentences = [sentence.strip() for sentence in re.split(r'(?<=[.!?])\\s+', text) if sentence.strip()]    #split string to sentences at '.', '!' or '?'\n",
        "text_data =sentences\n",
        "tokenizer = Tokenizer()     #‘Tokenizer’ object is created\n",
        "tokenizer.fit_on_texts(text_data)      # Analyzes the text and builds a vocabulary of unique words, assigning each word a numerical index\n",
        "total_words = len(tokenizer.word_index) + 1       # value of the length of the word index plus one (number of distinct words in the text)"
      ],
      "metadata": {
        "id": "Lx-fPfmmACCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences = []\n",
        "for line in text_data:         # the text data is split into lines using the ‘\\n’ character as a delimiter.\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]   # convert line into sequence of numerical tokens based on the previously created vocabulary\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1] # represents the input context, with last token being target or predicted word.\n",
        "        input_sequences.append(n_gram_sequence) # 'n-gram sequence' is appended to the ‘input_sequences’ list\n",
        "\n",
        "max_sequence_len = max([len(seq) for seq in input_sequences]) #assigned maximum length among all the input sequences\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')) #  used to pad or truncate input sequences to \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tmatch maximum length\n",
        "X = input_sequences[:, :-1] # assigned values of all rows in ‘input_sequences’ array except for the last column.\n",
        "y = input_sequences[:, -1] # values of the last column in the ‘input_sequences’ array, which represents the target or predicted word\n",
        "y = tf.keras.utils.to_categorical(y, num_classes=total_words) # convert target data to one-hot encoding"
      ],
      "metadata": {
        "id": "5vBI4MQBAFfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 10, input_length=max_sequence_len-1))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X, y, epochs=50, verbose=1)"
      ],
      "metadata": {
        "id": "HIpOiecTALFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_text = \"Pizza have made a\"\n",
        "token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "token_list = pad_sequences(\n",
        " [token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "predicted_probs = model.predict(token_list)\n",
        "predicted_word = tokenizer.index_word[np.argmax(predicted_probs)]\n",
        "seed_text += \" \" + predicted_word\n",
        "print(\"Next predicted word:\", seed_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxLaPTZdAPA6",
        "outputId": "673e6d93-5e73-4bce-e1ac-8af1527d9285"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 384ms/step\n",
            "Next predicted word: Pizza have made a symbol\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed_text = \"Pizza have made a\"\n",
        "next_words = 5\n",
        "for _ in range(next_words):\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences(\n",
        "        [token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "    predicted_probs = model.predict(token_list)\n",
        "    predicted_word = tokenizer.index_word[np.argmax(predicted_probs)]\n",
        "    seed_text += \" \" + predicted_word\n",
        "print(\"Next predicted words:\", seed_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdoYciXjATkR",
        "outputId": "2544e535-ff49-4f61-a751-f117e273fd27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "Next predicted words: Pizza have made a symbol of the shared of\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}